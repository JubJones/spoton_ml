"""
Phase 1 Detection Analysis Pipeline - Per-Scenario/Camera Analysis

This pipeline implements the Phase 1 requirements from ANALYSIS_PLANNING.md:
1. Single FasterRCNN model analysis across all scenarios/cameras
2. 1-per-person-ID failure collection with dual-color visualization
3. Performance mapping and environmental correlation analysis
4. Comprehensive reporting and actionable insights

Author: Generated by Claude Code
"""

import logging
import json
import csv
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional, Set
from collections import defaultdict, Counter
from dataclasses import dataclass, asdict
from datetime import datetime

import torch
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from PIL import Image, ImageDraw, ImageFont

from src.components.data.training_dataset import MTMMCDetectionDataset
from src.components.training.runner import get_transform, get_fasterrcnn_model
from src.components.training.rfdetr_runner import get_rfdetr_model

logger = logging.getLogger(__name__)

@dataclass
class FailureCase:
    """Represents a single detection failure case with comprehensive metadata."""
    person_id: int
    scene_id: str
    camera_id: str
    frame_path: str
    frame_number: int
    bbox_gt: List[float]  # Ground truth [x1, y1, x2, y2]
    bbox_pred: List[List[float]]  # Predicted boxes [[x1, y1, x2, y2], ...]
    scores_pred: List[float]  # Prediction scores
    model_name: str
    
    # Scene context
    lighting_condition: str
    crowd_density: str
    occlusion_level: str
    
    # Failure metadata
    iou_best: float  # Best IoU with any prediction
    confidence_best: float  # Confidence of best matching prediction
    gt_bbox_area: float
    frame_dimensions: Tuple[int, int]
    
@dataclass
class ScenePerformance:
    """Performance metrics for a specific scene/camera combination."""
    scene_id: str
    camera_id: str
    total_persons: int
    total_detections: int
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    map_score: float
    failures_by_lighting: Dict[str, int]
    failures_by_density: Dict[str, int]
    unique_failed_persons: Set[int]

class Phase1DetectionAnalyzer:
    """
    Phase 1 Detection Analysis Pipeline implementing ANALYSIS_PLANNING.md requirements.
    """
    
    def __init__(self, config: Dict[str, Any], device: torch.device):
        self.config = config
        self.device = device
        self.analysis_config = config.get("analysis", {})
        self.iou_threshold = self.analysis_config.get("iou_threshold", 0.5)
        self.confidence_threshold = self.analysis_config.get("confidence_threshold", 0.5)
        
        # Initialize output directories
        self.output_dir = Path(self.analysis_config.get("output_dir", "outputs/phase1_detection_analysis"))
        self.failure_images_dir = self.output_dir / "failure_images"
        self.reports_dir = self.output_dir / "reports"
        self.statistics_dir = self.output_dir / "statistics"
        
        # Create directories
        for dir_path in [self.failure_images_dir, self.reports_dir, self.statistics_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # Storage for analysis results
        self.failure_cases: List[FailureCase] = []
        self.scene_performances: List[ScenePerformance] = []
        self.collected_person_ids: Set[Tuple[str, str, int]] = set()  # (scene, camera, person_id)
        
        # Load dataset and model
        self.dataset = self._load_dataset()
        self.model = self._load_fasterrcnn_model()
        
        logger.info(f"Phase 1 Analysis initialized. Output directory: {self.output_dir}")
        
    def _load_dataset(self) -> MTMMCDetectionDataset:
        """Load the validation dataset for analysis."""
        logger.info("Loading validation dataset...")
        val_transforms = get_transform(train=False, config=self.config)
        return MTMMCDetectionDataset(
            config=self.config,
            mode='val',
            transforms=val_transforms
        )
    
    def _load_detection_model(self) -> torch.nn.Module:
        """Load the trained detection model from checkpoint (FasterRCNN or RF-DETR)."""
        model_type = self.config.get("model", {}).get("type", "fasterrcnn").lower()
        logger.info(f"Loading {model_type} model from checkpoint...")
        
        checkpoint_path = self.config.get("local_model_path")
        
        if model_type == "fasterrcnn":
            # Load FasterRCNN model architecture
            model = get_fasterrcnn_model(self.config)
            
            # Load weights from checkpoint
            if checkpoint_path and Path(checkpoint_path).exists():
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                
                # Handle different checkpoint formats
                if 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                    
                logger.info(f"Loaded FasterRCNN weights from: {checkpoint_path}")
            else:
                logger.warning(f"Checkpoint not found at: {checkpoint_path}. Using pre-trained weights.")
            
            model.to(self.device)
            model.eval()
            return model
            
        elif model_type == "rfdetr":
            # Load RF-DETR model architecture
            model = get_rfdetr_model(self.config)
            
            # Load weights from checkpoint if available
            if checkpoint_path and Path(checkpoint_path).exists():
                try:
                    # RF-DETR models may have different checkpoint formats
                    checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                    
                    # Handle different RF-DETR checkpoint formats
                    if hasattr(model, 'model') and hasattr(model.model, 'load_state_dict'):
                        if 'model_state_dict' in checkpoint:
                            model.model.load_state_dict(checkpoint['model_state_dict'])
                        elif 'model' in checkpoint:
                            model.model.load_state_dict(checkpoint['model'])
                        else:
                            model.model.load_state_dict(checkpoint)
                    else:
                        logger.warning("RF-DETR checkpoint loading format not recognized, using default weights")
                        
                    logger.info(f"Loaded RF-DETR weights from: {checkpoint_path}")
                except Exception as e:
                    logger.warning(f"Failed to load RF-DETR checkpoint: {e}. Using default weights.")
            else:
                logger.warning(f"Checkpoint not found at: {checkpoint_path}. Using pre-trained weights.")
            
            # RF-DETR models handle device internally
            return model
            
        else:
            raise ValueError(f"Unsupported model type: {model_type}. Supported types: 'fasterrcnn', 'rfdetr'")
    
    def _load_fasterrcnn_model(self) -> torch.nn.Module:
        """Legacy method for backward compatibility."""
        return self._load_detection_model()
    
    def run_complete_analysis(self) -> None:
        """Run the complete Phase 1 analysis pipeline."""
        logger.info("Starting Phase 1: Per-Scenario/Camera Detection Analysis")
        
        # Get all unique scene/camera pairs
        scene_camera_pairs = self._get_scene_camera_pairs()
        logger.info(f"Found {len(scene_camera_pairs)} scene/camera combinations")
        
        # Analyze each scene/camera combination
        for scene_id, camera_id in scene_camera_pairs:
            logger.info(f"Analyzing scene: {scene_id}, camera: {camera_id}")
            self._analyze_scene_camera(scene_id, camera_id)
            
        # Generate outputs
        self._generate_failure_visualizations()
        self._generate_performance_reports()
        self._generate_statistical_analysis()
        
        logger.info("Phase 1 analysis completed successfully")
        logger.info(f"Results saved to: {self.output_dir}")
    
    def _get_scene_camera_pairs(self) -> List[Tuple[str, str]]:
        """Get all unique scene/camera pairs from the dataset."""
        pairs = set()
        for i in range(len(self.dataset)):
            info = self.dataset.get_sample_info(i)
            if info:
                pairs.add((info['scene_id'], info['camera_id']))
        return sorted(list(pairs))
    
    def _analyze_scene_camera(self, scene_id: str, camera_id: str) -> None:
        """Analyze a specific scene/camera combination."""
        
        # Get indices for this scene/camera
        camera_indices = []
        for i in range(len(self.dataset)):
            info = self.dataset.get_sample_info(i)
            if info and info['scene_id'] == scene_id and info['camera_id'] == camera_id:
                camera_indices.append(i)
        
        if not camera_indices:
            logger.warning(f"No data found for {scene_id}/{camera_id}")
            return
        
        # Track performance metrics
        total_persons = 0
        total_detections = 0
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        failures_by_lighting = defaultdict(int)
        failures_by_density = defaultdict(int)
        unique_failed_persons = set()
        
        # Process each frame
        for dataset_idx in tqdm(camera_indices, desc=f"Processing {scene_id}/{camera_id}"):
            frame_result = self._process_frame(dataset_idx, scene_id, camera_id)
            
            if frame_result:
                # Update metrics
                total_persons += frame_result['total_gt_objects']
                total_detections += frame_result['total_predictions']
                true_positives += frame_result['true_positives']
                false_positives += frame_result['false_positives']
                false_negatives += frame_result['false_negatives']
                
                # Update failure statistics
                for failure in frame_result['failures']:
                    failures_by_lighting[failure.lighting_condition] += 1
                    failures_by_density[failure.crowd_density] += 1
                    unique_failed_persons.add(failure.person_id)
                    
                    # Check if we should collect this failure (1 per person ID)
                    person_key = (scene_id, camera_id, failure.person_id)
                    if person_key not in self.collected_person_ids:
                        self.failure_cases.append(failure)
                        self.collected_person_ids.add(person_key)
                        logger.debug(f"Collected failure for person {failure.person_id} in {scene_id}/{camera_id}")
        
        # Calculate performance metrics
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        map_score = recall  # Simplified mAP approximation
        
        # Store scene performance
        scene_performance = ScenePerformance(
            scene_id=scene_id,
            camera_id=camera_id,
            total_persons=total_persons,
            total_detections=total_detections,
            true_positives=true_positives,
            false_positives=false_positives,
            false_negatives=false_negatives,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            map_score=map_score,
            failures_by_lighting=dict(failures_by_lighting),
            failures_by_density=dict(failures_by_density),
            unique_failed_persons=unique_failed_persons
        )
        
        self.scene_performances.append(scene_performance)
        logger.info(f"Completed {scene_id}/{camera_id}: P={precision:.3f}, R={recall:.3f}, F1={f1_score:.3f}")
    
    def _process_frame(self, dataset_idx: int, scene_id: str, camera_id: str) -> Optional[Dict]:
        """Process a single frame and return analysis results."""
        
        try:
            # Get frame data
            image_tensor, target = self.dataset[dataset_idx]
            image_path = self.dataset.get_image_path(dataset_idx)
            
            if not image_path:
                return None
            
            # Load original image for visualization
            original_image = cv2.imread(str(image_path))
            if original_image is None:
                return None
            
            # Run detection based on model type
            model_type = self.config.get("model", {}).get("type", "fasterrcnn").lower()
            
            if model_type == "fasterrcnn":
                # Run FasterRCNN detection
                with torch.no_grad():
                    prediction = self.model([image_tensor.to(self.device)])[0]
                    pred_boxes = prediction['boxes'].cpu()
                    pred_scores = prediction['scores'].cpu()
                    pred_labels = prediction['labels'].cpu()
                
                # Filter for person class (assuming class 1 is person)
                person_mask = pred_labels == 1
                pred_boxes = pred_boxes[person_mask]
                pred_scores = pred_scores[person_mask]
                
            elif model_type == "rfdetr":
                # Run RF-DETR detection
                with torch.no_grad():
                    # Convert tensor to PIL image for RF-DETR
                    image_pil = Image.fromarray(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))
                    results = self.model.predict(image_pil)
                    
                    # Extract predictions from RF-DETR results
                    pred_boxes = []
                    pred_scores = []
                    
                    if results and hasattr(results, 'boxes') and results.boxes is not None:
                        # RF-DETR results format
                        boxes = results.boxes.xyxy.cpu()  # x1, y1, x2, y2
                        scores = results.boxes.conf.cpu()
                        labels = results.boxes.cls.cpu()
                        
                        # Filter for person class (class 0 in RF-DETR)
                        person_mask = labels == 0
                        pred_boxes = boxes[person_mask]
                        pred_scores = scores[person_mask]
                    else:
                        # Empty results
                        pred_boxes = torch.empty((0, 4))
                        pred_scores = torch.empty((0,))
            
            else:
                raise ValueError(f"Unsupported model type: {model_type}")
            
            # Convert to tensors if needed
            if not isinstance(pred_boxes, torch.Tensor):
                pred_boxes = torch.tensor(pred_boxes) if len(pred_boxes) > 0 else torch.empty((0, 4))
            if not isinstance(pred_scores, torch.Tensor):
                pred_scores = torch.tensor(pred_scores) if len(pred_scores) > 0 else torch.empty((0,))
            
            # Filter by confidence threshold
            conf_mask = pred_scores >= self.confidence_threshold
            pred_boxes = pred_boxes[conf_mask]
            pred_scores = pred_scores[conf_mask]
            
            # Get ground truth
            gt_boxes = target['boxes'].cpu()
            gt_ids = target['labels'].cpu()
            
            # Analyze scene conditions
            scene_conditions = self._analyze_scene_conditions(original_image)
            
            # Calculate metrics and collect failures
            true_positives = 0
            false_positives = len(pred_boxes)
            false_negatives = 0
            failures = []
            
            # For each ground truth person
            for i, (gt_box, person_id) in enumerate(zip(gt_boxes, gt_ids)):
                person_id = int(person_id.item())
                
                # Calculate IoU with all predictions
                best_iou = 0.0
                best_confidence = 0.0
                
                if len(pred_boxes) > 0:
                    ious = self._calculate_iou(gt_box.unsqueeze(0), pred_boxes)
                    best_iou = torch.max(ious).item()
                    best_idx = torch.argmax(ious).item()
                    best_confidence = pred_scores[best_idx].item()
                
                # Check if detected
                if best_iou >= self.iou_threshold:
                    true_positives += 1
                    false_positives -= 1
                else:
                    false_negatives += 1
                    
                    # Create failure case
                    failure = FailureCase(
                        person_id=person_id,
                        scene_id=scene_id,
                        camera_id=camera_id,
                        frame_path=str(image_path),
                        frame_number=dataset_idx,
                        bbox_gt=gt_box.tolist(),
                        bbox_pred=pred_boxes.tolist(),
                        scores_pred=pred_scores.tolist(),
                        model_name=f"{model_type}_trained",
                        lighting_condition=scene_conditions['lighting'],
                        crowd_density=scene_conditions['crowd_density'],
                        occlusion_level=scene_conditions['occlusion'],
                        iou_best=best_iou,
                        confidence_best=best_confidence,
                        gt_bbox_area=self._calculate_area(gt_box),
                        frame_dimensions=(original_image.shape[1], original_image.shape[0])
                    )
                    failures.append(failure)
            
            return {
                'total_gt_objects': len(gt_ids),
                'total_predictions': len(pred_boxes),
                'true_positives': true_positives,
                'false_positives': false_positives,
                'false_negatives': false_negatives,
                'failures': failures,
                'scene_conditions': scene_conditions
            }
            
        except Exception as e:
            logger.error(f"Error processing frame {dataset_idx}: {e}")
            return None
    
    def _analyze_scene_conditions(self, image: np.ndarray) -> Dict[str, str]:
        """Analyze scene conditions from image."""
        
        # Basic lighting analysis
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        mean_brightness = np.mean(gray)
        
        if mean_brightness < 60:
            lighting = 'night'
        elif mean_brightness > 120:
            lighting = 'day'
        else:
            lighting = 'transition'
        
        # Crowd density estimation
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / (edges.shape[0] * edges.shape[1])
        
        if edge_density < 0.1:
            crowd_density = 'low'
        elif edge_density < 0.3:
            crowd_density = 'medium'
        else:
            crowd_density = 'high'
        
        # Simplified occlusion analysis
        occlusion = 'partial'  # Default assumption
        
        return {
            'lighting': lighting,
            'crowd_density': crowd_density,
            'occlusion': occlusion
        }
    
    def _calculate_iou(self, box1: torch.Tensor, box2: torch.Tensor) -> torch.Tensor:
        """Calculate IoU between two sets of boxes."""
        area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])
        area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])
        
        # Intersection coordinates
        x1 = torch.max(box1[:, 0:1], box2[:, 0:1].T)
        y1 = torch.max(box1[:, 1:2], box2[:, 1:2].T)
        x2 = torch.min(box1[:, 2:3], box2[:, 2:3].T)
        y2 = torch.min(box1[:, 3:4], box2[:, 3:4].T)
        
        # Intersection area
        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)
        
        # Union area
        union = area1.unsqueeze(1) + area2.unsqueeze(0) - intersection
        
        # IoU
        iou = intersection / torch.clamp(union, min=1e-6)
        return iou
    
    def _calculate_area(self, box: torch.Tensor) -> float:
        """Calculate area of a bounding box."""
        return ((box[2] - box[0]) * (box[3] - box[1])).item()
    
    def _generate_failure_visualizations(self) -> None:
        """Generate failure visualizations with dual-color bounding boxes."""
        logger.info("Generating failure visualizations with dual-color bounding boxes...")
        
        for failure in tqdm(self.failure_cases, desc="Creating failure visualizations"):
            self._create_dual_color_failure_image(failure)
    
    def _create_dual_color_failure_image(self, failure: FailureCase) -> None:
        """Create failure image with RED ground truth and BLUE detected boxes."""
        
        try:
            # Load original image
            image = cv2.imread(failure.frame_path)
            if image is None:
                logger.error(f"Could not load image: {failure.frame_path}")
                return
            
            # Convert to RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Create figure
            fig, ax = plt.subplots(1, 1, figsize=(12, 8))
            ax.imshow(image_rgb)
            
            # Draw ground truth box in RED
            x1, y1, x2, y2 = failure.bbox_gt
            gt_rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, 
                                  fill=False, color='red', linewidth=3, label='Ground Truth (Missed)')
            ax.add_patch(gt_rect)
            
            # Add ground truth label
            ax.text(x1, y1-10, f"GT: Person {failure.person_id}", 
                   color='red', fontsize=12, weight='bold',
                   bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
            
            # Draw detected boxes in BLUE
            for i, (pred_box, score) in enumerate(zip(failure.bbox_pred, failure.scores_pred)):
                px1, py1, px2, py2 = pred_box
                pred_rect = plt.Rectangle((px1, py1), px2-px1, py2-py1, 
                                        fill=False, color='blue', linewidth=2, 
                                        label='Detected' if i == 0 else "")
                ax.add_patch(pred_rect)
                
                # Add detection label
                ax.text(px1, py1-10, f"Det: {score:.2f}", 
                       color='blue', fontsize=10,
                       bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.8))
            
            # Add title and info
            ax.set_title(f"Detection Failure - {failure.scene_id}/{failure.camera_id} - Person {failure.person_id}", 
                        fontsize=14, weight='bold')
            ax.axis('off')
            
            # Add legend
            ax.legend(loc='upper right')
            
            # Save the visualization
            filename = f"failure_{failure.scene_id}_{failure.camera_id}_person{failure.person_id}.png"
            output_path = self.failure_images_dir / filename
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            logger.error(f"Error creating failure visualization: {e}")
    
    def _generate_performance_reports(self) -> None:
        """Generate comprehensive performance reports."""
        logger.info("Generating performance reports...")
        
        # Generate performance matrix
        self._generate_performance_matrix()
        
        # Generate HTML report
        self._generate_html_report()
        
        # Generate best/worst environment analysis
        self._generate_environment_analysis()
    
    def _generate_performance_matrix(self) -> None:
        """Generate performance matrix CSV."""
        
        performance_data = []
        for perf in self.scene_performances:
            performance_data.append({
                'scene_id': perf.scene_id,
                'camera_id': perf.camera_id,
                'precision': perf.precision,
                'recall': perf.recall,
                'f1_score': perf.f1_score,
                'map_score': perf.map_score,
                'total_persons': perf.total_persons,
                'total_detections': perf.total_detections,
                'true_positives': perf.true_positives,
                'false_positives': perf.false_positives,
                'false_negatives': perf.false_negatives,
                'unique_failed_persons': len(perf.unique_failed_persons),
                'day_failures': perf.failures_by_lighting.get('day', 0),
                'night_failures': perf.failures_by_lighting.get('night', 0),
                'transition_failures': perf.failures_by_lighting.get('transition', 0),
                'low_density_failures': perf.failures_by_density.get('low', 0),
                'medium_density_failures': perf.failures_by_density.get('medium', 0),
                'high_density_failures': perf.failures_by_density.get('high', 0),
            })
        
        df = pd.DataFrame(performance_data)
        df.to_csv(self.statistics_dir / "scenario_performance_matrix.csv", index=False)
        logger.info("Performance matrix saved to scenario_performance_matrix.csv")
    
    def _generate_html_report(self) -> None:
        """Generate comprehensive HTML report."""
        
        # Calculate overall statistics
        total_failures = len(self.failure_cases)
        total_scenarios = len(self.scene_performances)
        avg_precision = np.mean([p.precision for p in self.scene_performances])
        avg_recall = np.mean([p.recall for p in self.scene_performances])
        avg_f1 = np.mean([p.f1_score for p in self.scene_performances])
        
        # Find best and worst performing scenarios
        best_scenario = max(self.scene_performances, key=lambda x: x.f1_score)
        worst_scenario = min(self.scene_performances, key=lambda x: x.f1_score)
        
        # Generate HTML content
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Phase 1 Detection Analysis Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border-left: 4px solid #007acc; }}
                .metric {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-radius: 3px; }}
                .good {{ background-color: #e6ffe6; }}
                .bad {{ background-color: #ffe6e6; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Phase 1: Per-Scenario/Camera Detection Analysis Report</h1>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p>Model: {self.config.get("model", {}).get("type", "fasterrcnn").upper()} (trained)</p>
                <p>Checkpoint: {self.config.get('local_model_path', 'N/A')}</p>
            </div>
            
            <div class="section">
                <h2>Executive Summary</h2>
                <div class="metric">
                    <strong>Total Scenarios Analyzed:</strong> {total_scenarios}
                </div>
                <div class="metric">
                    <strong>Total Unique Failures Collected:</strong> {total_failures}
                </div>
                <div class="metric">
                    <strong>Average Precision:</strong> {avg_precision:.3f}
                </div>
                <div class="metric">
                    <strong>Average Recall:</strong> {avg_recall:.3f}
                </div>
                <div class="metric">
                    <strong>Average F1 Score:</strong> {avg_f1:.3f}
                </div>
            </div>
            
            <div class="section">
                <h2>Best and Worst Performing Environments</h2>
                
                <div class="metric good">
                    <h3>Best Performing: {best_scenario.scene_id}/{best_scenario.camera_id}</h3>
                    <p>Precision: {best_scenario.precision:.3f} | Recall: {best_scenario.recall:.3f} | F1: {best_scenario.f1_score:.3f}</p>
                </div>
                
                <div class="metric bad">
                    <h3>Worst Performing: {worst_scenario.scene_id}/{worst_scenario.camera_id}</h3>
                    <p>Precision: {worst_scenario.precision:.3f} | Recall: {worst_scenario.recall:.3f} | F1: {worst_scenario.f1_score:.3f}</p>
                </div>
            </div>
            
            <div class="section">
                <h2>Environmental Analysis</h2>
                <h3>Failures by Lighting Condition</h3>
                {self._generate_failure_breakdown_html()}
            </div>
            
            <div class="section">
                <h2>Key Findings</h2>
                <ul>
                    <li>Analyzed {total_scenarios} different scenario/camera combinations</li>
                    <li>Collected {total_failures} unique failure cases (1 per person ID)</li>
                    <li>Best performance in {best_scenario.scene_id}/{best_scenario.camera_id} with F1={best_scenario.f1_score:.3f}</li>
                    <li>Worst performance in {worst_scenario.scene_id}/{worst_scenario.camera_id} with F1={worst_scenario.f1_score:.3f}</li>
                    <li>Failure images saved with dual-color visualization (RED=GT, BLUE=Detected)</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>Recommendations</h2>
                <ul>
                    <li>Focus improvement efforts on {worst_scenario.scene_id}/{worst_scenario.camera_id} scenario</li>
                    <li>Review training data for underrepresented lighting conditions</li>
                    <li>Consider data augmentation for poor-performing scenarios</li>
                    <li>Investigate camera-specific calibration issues</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        with open(self.reports_dir / "phase1_analysis_report.html", "w") as f:
            f.write(html_content)
        
        logger.info("HTML report generated: phase1_analysis_report.html")
    
    def _generate_failure_breakdown_html(self) -> str:
        """Generate HTML breakdown of failures by environmental factors."""
        
        # Aggregate failures by lighting condition
        lighting_failures = defaultdict(int)
        density_failures = defaultdict(int)
        
        for failure in self.failure_cases:
            lighting_failures[failure.lighting_condition] += 1
            density_failures[failure.crowd_density] += 1
        
        html = "<h4>Lighting Conditions</h4><ul>"
        for condition, count in sorted(lighting_failures.items()):
            html += f"<li>{condition.title()}: {count} failures</li>"
        html += "</ul>"
        
        html += "<h4>Crowd Density</h4><ul>"
        for density, count in sorted(density_failures.items()):
            html += f"<li>{density.title()}: {count} failures</li>"
        html += "</ul>"
        
        return html
    
    def _generate_environment_analysis(self) -> None:
        """Generate detailed environment analysis."""
        
        # Sort scenarios by performance
        sorted_scenarios = sorted(self.scene_performances, key=lambda x: x.f1_score, reverse=True)
        
        # Generate environment analysis
        analysis_data = {
            'best_environments': [],
            'worst_environments': [],
            'lighting_analysis': defaultdict(list),
            'density_analysis': defaultdict(list)
        }
        
        # Top 5 and bottom 5 scenarios
        for i, scenario in enumerate(sorted_scenarios[:5]):
            analysis_data['best_environments'].append({
                'rank': i + 1,
                'scene_camera': f"{scenario.scene_id}/{scenario.camera_id}",
                'f1_score': scenario.f1_score,
                'precision': scenario.precision,
                'recall': scenario.recall
            })
        
        for i, scenario in enumerate(sorted_scenarios[-5:]):
            analysis_data['worst_environments'].append({
                'rank': len(sorted_scenarios) - i,
                'scene_camera': f"{scenario.scene_id}/{scenario.camera_id}",
                'f1_score': scenario.f1_score,
                'precision': scenario.precision,
                'recall': scenario.recall
            })
        
        # Analyze by environmental factors
        for failure in self.failure_cases:
            analysis_data['lighting_analysis'][failure.lighting_condition].append(failure)
            analysis_data['density_analysis'][failure.crowd_density].append(failure)
        
        # Save analysis
        with open(self.reports_dir / "environment_analysis.json", "w") as f:
            # Convert to serializable format
            serializable_data = {
                'best_environments': analysis_data['best_environments'],
                'worst_environments': analysis_data['worst_environments'],
                'lighting_analysis': {k: len(v) for k, v in analysis_data['lighting_analysis'].items()},
                'density_analysis': {k: len(v) for k, v in analysis_data['density_analysis'].items()}
            }
            json.dump(serializable_data, f, indent=2)
        
        logger.info("Environment analysis saved to environment_analysis.json")
    
    def _generate_statistical_analysis(self) -> None:
        """Generate statistical analysis and visualizations."""
        logger.info("Generating statistical analysis...")
        
        # Export detailed failure data
        failure_data = []
        for failure in self.failure_cases:
            failure_data.append({
                'person_id': failure.person_id,
                'scene_id': failure.scene_id,
                'camera_id': failure.camera_id,
                'lighting_condition': failure.lighting_condition,
                'crowd_density': failure.crowd_density,
                'occlusion_level': failure.occlusion_level,
                'iou_best': failure.iou_best,
                'confidence_best': failure.confidence_best,
                'gt_bbox_area': failure.gt_bbox_area,
                'frame_width': failure.frame_dimensions[0],
                'frame_height': failure.frame_dimensions[1]
            })
        
        df_failures = pd.DataFrame(failure_data)
        df_failures.to_csv(self.statistics_dir / "failure_cases.csv", index=False)
        
        # Generate visualizations
        self._create_performance_visualizations()
        
        logger.info("Statistical analysis completed")
    
    def _create_performance_visualizations(self) -> None:
        """Create performance visualization plots."""
        
        # Set up plotting style
        plt.style.use('seaborn-v0_8')
        
        # Performance by scenario
        scenarios = [f"{p.scene_id}/{p.camera_id}" for p in self.scene_performances]
        f1_scores = [p.f1_score for p in self.scene_performances]
        precisions = [p.precision for p in self.scene_performances]
        recalls = [p.recall for p in self.scene_performances]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # F1 scores by scenario
        axes[0, 0].bar(range(len(scenarios)), f1_scores)
        axes[0, 0].set_title('F1 Score by Scenario/Camera')
        axes[0, 0].set_xlabel('Scenario/Camera')
        axes[0, 0].set_ylabel('F1 Score')
        axes[0, 0].set_xticks(range(len(scenarios)))
        axes[0, 0].set_xticklabels(scenarios, rotation=45)
        
        # Precision vs Recall scatter
        axes[0, 1].scatter(precisions, recalls, alpha=0.7)
        axes[0, 1].set_xlabel('Precision')
        axes[0, 1].set_ylabel('Recall')
        axes[0, 1].set_title('Precision vs Recall by Scenario')
        
        # Failures by lighting condition
        if self.failure_cases:
            lighting_counts = Counter(f.lighting_condition for f in self.failure_cases)
            axes[1, 0].bar(lighting_counts.keys(), lighting_counts.values())
            axes[1, 0].set_title('Failures by Lighting Condition')
            axes[1, 0].set_ylabel('Number of Failures')
        
        # Failures by crowd density
        if self.failure_cases:
            density_counts = Counter(f.crowd_density for f in self.failure_cases)
            axes[1, 1].bar(density_counts.keys(), density_counts.values())
            axes[1, 1].set_title('Failures by Crowd Density')
            axes[1, 1].set_ylabel('Number of Failures')
        
        plt.tight_layout()
        plt.savefig(self.statistics_dir / "performance_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info("Performance visualizations saved to performance_analysis.png")


def run_phase1_analysis(config: Dict[str, Any], device: torch.device) -> None:
    """Main entry point for Phase 1 analysis."""
    
    analyzer = Phase1DetectionAnalyzer(config, device)
    analyzer.run_complete_analysis()
    
    logger.info("Phase 1 Detection Analysis completed successfully!")
    logger.info(f"Check results in: {analyzer.output_dir}")